{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python391jvsc74a57bd046be4515178f7fa5b6715b984804b00918b5181a0e3d72891a8dc4ff728ede02",
   "display_name": "Python 3.9.1 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "46be4515178f7fa5b6715b984804b00918b5181a0e3d72891a8dc4ff728ede02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'train_dataset.txt'\n",
    "dev_path = 'dev_dataset.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24273/24273 [00:40<00:00, 594.23it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8285/8285 [00:23<00:00, 352.44it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24205/24205 [00:03<00:00, 6868.56it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8268/8268 [00:01<00:00, 6540.70it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2180411"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "train_text = ''\n",
    "dev_text = ''\n",
    "train_file = open(train_path, 'w')\n",
    "dev_file = open(dev_path, 'w')\n",
    "for i in tqdm(range(1,24274)):\n",
    "    try:\n",
    "        f = open(\"C:\\\\Users\\\\IvanH\\\\gaokao_mcq_answering\\\\train\\\\high\\\\\" + str(i)+ \".txt\", \"r\")\n",
    "    except:\n",
    "        continue\n",
    "    train_text += json.loads(f.read())['article'] + '\\n'\n",
    "    f.close()\n",
    "for i in tqdm(range(1,8286)):\n",
    "    try:\n",
    "        f = open(\"C:\\\\Users\\\\IvanH\\\\gaokao_mcq_answering\\\\train\\\\middle\\\\\" + str(i)+ \".txt\", \"r\")\n",
    "    except:\n",
    "        continue\n",
    "    train_text += json.loads(f.read())['article'] + '\\n'\n",
    "    f.close()\n",
    "\n",
    "for i in tqdm(range(63,24268)):\n",
    "    try:\n",
    "        f = open(\"C:\\\\Users\\\\IvanH\\\\gaokao_mcq_answering\\\\dev\\\\high\\\\\" + str(i)+ \".txt\", \"r\")\n",
    "    except:\n",
    "        continue\n",
    "    dev_text += json.loads(f.read())['article'] + '\\n'\n",
    "    f.close()\n",
    "for i in tqdm(range(13,8281)):\n",
    "    try:\n",
    "        f = open(\"C:\\\\Users\\\\IvanH\\\\gaokao_mcq_answering\\\\dev\\\\middle\\\\\" + str(i)+ \".txt\", \"r\")\n",
    "    except:\n",
    "        continue\n",
    "    dev_text += json.loads(f.read())['article'] + '\\n'\n",
    "    f.close()\n",
    "train_file.write(train_text)\n",
    "dev_file.write(dev_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast,GPT2LMHeadModel\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')"
   ]
  },
  {
   "source": [
    "Training codes are partially adapted from https://github.com/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\IvanH\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "\n",
    "def load_dataset(dev_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=32)\n",
    "        #   block_size=128)\n",
    "     \n",
    "    dev_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=dev_path,\n",
    "          block_size=32) \n",
    "        #   block_size=128)  \n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,dev_dataset,data_collator\n",
    "\n",
    "train_dataset,dev_dataset,data_collator = load_dataset(train_path,dev_path,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=3, # number of training epochs\n",
    "    per_device_train_batch_size=32, # batch size for training\n",
    "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "    eval_steps = 400, # Number of update steps between two evaluations.\n",
    "    save_steps=800, # after # steps model is saved \n",
    "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
    "    prediction_loss_only=True,\n",
    "    )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 500/849 [02:44<01:55,  3.02it/s]{'loss': 3.8955, 'learning_rate': 5e-05, 'epoch': 1.77}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 849/849 [04:41<00:00,  3.01it/s]\n",
      "{'train_runtime': 281.9421, 'train_samples_per_second': 3.011, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                            question                      A  \\\n",
       "0   I can't remember _____________ made the teach...   that it was what       \n",
       "1     He let out an ______________ cry, \"we've won!\"        excited           \n",
       "2   Is football John's favourite sport? Yes. ____...        Near to           \n",
       "3   Do you think regular exercise ___________ goo...     benefit from         \n",
       "4   Have you applied ___________ Mr Black _______...      for; to             \n",
       "\n",
       "                       B                      C                  D Answer  \n",
       "0   what it was that       what was it that       that was it what      B  \n",
       "1        exciting                excite                    excites      A  \n",
       "2         Except                  Beside                   Next to      D  \n",
       "3       reach for                make for                   go for      C  \n",
       "4      with; for              with; about                  to; for      D  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>A</th>\n      <th>B</th>\n      <th>C</th>\n      <th>D</th>\n      <th>Answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I can't remember _____________ made the teach...</td>\n      <td>that it was what</td>\n      <td>what it was that</td>\n      <td>what was it that</td>\n      <td>that was it what</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>He let out an ______________ cry, \"we've won!\"</td>\n      <td>excited</td>\n      <td>exciting</td>\n      <td>excite</td>\n      <td>excites</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Is football John's favourite sport? Yes. ____...</td>\n      <td>Near to</td>\n      <td>Except</td>\n      <td>Beside</td>\n      <td>Next to</td>\n      <td>D</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Do you think regular exercise ___________ goo...</td>\n      <td>benefit from</td>\n      <td>reach for</td>\n      <td>make for</td>\n      <td>go for</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Have you applied ___________ Mr Black _______...</td>\n      <td>for; to</td>\n      <td>with; for</td>\n      <td>with; about</td>\n      <td>to; for</td>\n      <td>D</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "import pandas as pd \n",
    "data_df = pd.read_csv('data.csv')\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    " \n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "# Load pre-trained model (weights)\n",
    "with torch.no_grad():\n",
    "        model = GPT2LMHeadModel.from_pretrained('output')\n",
    "        model.eval()\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    " \n",
    "def score(sentence):\n",
    "    tokenize_input = tokenizer.encode(sentence)\n",
    "    tensor_input = torch.tensor([tokenize_input])\n",
    "    loss=model(tensor_input, labels=tensor_input)[0]\n",
    "    return np.exp(loss.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1183/1183 [06:36<00:00,  2.98it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "prediction_list=[]\n",
    "answer_list= []\n",
    "for i in tqdm(range(len(data_df))):\n",
    "    row = data_df.iloc[i]\n",
    "    question_original = row['question'].strip()\n",
    "    choice_list = ['A','B','C','D']\n",
    "    score_list = []\n",
    "    sentence_list = []\n",
    "\n",
    "    if ';' in row['A']:\n",
    "        try:\n",
    "            for choice in choice_list:\n",
    "                choice_split = [i.strip() for i in row[choice].split(';')]\n",
    "                if len(choice_split) == 2:\n",
    "                    question_text = re.sub('\\_+',' '+choice_split[0]+' ', question_original,1)\n",
    "                    question_text = re.sub('\\_+',' '+choice_split[1]+' ', question_text,1)\n",
    "                if len(choice_split) == 3:\n",
    "                    question_text = re.sub('\\_+',' '+choice_split[0]+' ', question_original,1)\n",
    "                    question_text = re.sub('\\_+',' '+choice_split[1]+' ', question_text,1)\n",
    "                    question_text = re.sub('\\_+',' '+choice_split[2]+' ', question_text,1)\n",
    "                sentence_list.append(question_text)\n",
    "        except:\n",
    "                print(question_original,question_text,choice_split)\n",
    "                continue\n",
    "    else:\n",
    "        sentence_list = [re.sub('\\_+',' '+row[choice].strip()+' ', question_original) for choice in choice_list]\n",
    "\n",
    "    if ('/' in row['A']+row['B']+row['C']+row['D']):\n",
    "        sentence_list = [re.sub('/','',sentence) for sentence in sentence_list]\n",
    "\n",
    "    sentence_list = [re.sub(' +',' ', sentence) for sentence in sentence_list]\n",
    "\n",
    "    for sentence in sentence_list:\n",
    "        score_list.append(score(sentence))\n",
    "    final_choice = choice_list[np.argmin(np.array(score_list))]\n",
    "    answer_list.append(final_choice)\n",
    "    if final_choice == row['Answer']:\n",
    "        prediction_list.append(1)\n",
    "    else:\n",
    "        prediction_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6830092983939138"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "np.array(prediction_list).sum()/len(prediction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}